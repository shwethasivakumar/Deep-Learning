{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "89794556-b2d5-4471-b97d-a100a08bc549",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/lib/python3.11/site-packages/transformers/utils/generic.py:260: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from transformers import BertModel, BertTokenizerFast, AdamW\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd325c82-949a-4322-b9ba-86710ca500b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_data(path): \n",
    "    \n",
    "    with open(path, 'rb') as f:\n",
    "        raw_data = json.load(f)\n",
    "    \n",
    "    contexts_train = []\n",
    "    questions_train = []\n",
    "    answers_train = []\n",
    "    num_q_train = 0\n",
    "    num_pos_train = 0\n",
    "    num_imp_train = 0\n",
    "    \n",
    "    for group in raw_data['data']:\n",
    "        for paragraph in group['paragraphs']:\n",
    "            context_train = paragraph['context']\n",
    "            for qa in paragraph['qas']:\n",
    "                question_train = qa['question']\n",
    "                num_q_train += 1\n",
    "                for answer in qa['answers']:\n",
    "                    contexts_train.append(context_train.lower())\n",
    "                    questions_train.append(question_train.lower())\n",
    "                    answers_train.append(answer)\n",
    "    \n",
    "    return num_q_train, num_pos_train, num_imp_train, contexts_train, questions_train, answers_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "93818501-8012-41d8-aeec-8542be32967a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_of_questions_train = 0\n",
    "num_of_possible_train = 0\n",
    "num_of_impossible_train = 0\n",
    "num_q_train, num_pos_train, num_imp_train, train_contexts_train, train_questions_train, train_answers_train = get_data('spoken_train-v1.1.json')\n",
    "num_of_questions_train = num_q_train\n",
    "num_of_possible_train = num_pos_train\n",
    "num_of_impossible_train = num_imp_train\n",
    "\n",
    "num_q_valid, num_pos_valid, num_imp_valid, valid_contexts_valid, valid_questions_valid, valid_answers_valid = get_data('spoken_test-v1.1.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1980c6cd-9974-47f6-a325-accad1a8454e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shwetha/.local/lib/python3.11/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "def add_answer_at_end(answers_train, contexts_train):\n",
    "    for answer, context in zip(answers_train, contexts_train):\n",
    "        answer['text'] = answer['text'].lower()\n",
    "        answer['answer_end'] = answer['answer_start'] + len(answer['text'])\n",
    "\n",
    "add_answer_at_end(train_answers_train, train_contexts_train)\n",
    "add_answer_at_end(valid_answers_valid, valid_contexts_valid)\n",
    "\n",
    "\n",
    "MAX_LENGTH = 250\n",
    "MODEL_PATH = \"bert-base-uncased\"\n",
    "\n",
    "tokenizerFast = BertTokenizerFast.from_pretrained(MODEL_PATH)\n",
    "train_encodings_fast = tokenizerFast(train_questions_train, train_contexts_train, max_length=MAX_LENGTH, truncation=True, padding=True)\n",
    "valid_encodings_fast = tokenizerFast(valid_questions_valid, valid_contexts_valid, max_length=MAX_LENGTH, truncation=True, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "49850c61-c376-412e-ae97-c3a827e4aa42",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def return_Answer_startandend_train(idx):\n",
    "    return_start = 0\n",
    "    return_end = 0\n",
    "    answer_encoding_fast = tokenizerFast(train_answers_train[idx]['text'], max_length=MAX_LENGTH, truncation=True, padding=True)\n",
    "    \n",
    "    for a in range(len(train_encodings_fast['input_ids'][idx]) - len(answer_encoding_fast['input_ids'])): \n",
    "        match = True\n",
    "        for i in range(1, len(answer_encoding_fast['input_ids']) - 1):\n",
    "            if answer_encoding_fast['input_ids'][i] != train_encodings_fast['input_ids'][idx][a + i]:\n",
    "                match = False\n",
    "                break\n",
    "            if match:\n",
    "                return_start = a + 1\n",
    "                return_end = a + i + 1\n",
    "                break\n",
    "    return (return_start, return_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7b5f8b02-8038-4a4a-b78d-6cffb7165f1d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "478\n"
     ]
    }
   ],
   "source": [
    "start_positions_train = []\n",
    "end_positions_train = []\n",
    "counter_train = 0\n",
    "\n",
    "for t in range(len(train_encodings_fast['input_ids'])):\n",
    "    s, e = return_Answer_startandend_train(t)\n",
    "    start_positions_train.append(s)\n",
    "    end_positions_train.append(e)\n",
    "    \n",
    "    if s == 0:\n",
    "        counter_train += 1\n",
    "\n",
    "train_encodings_fast.update({'start_positions': start_positions_train, 'end_positions': end_positions_train})\n",
    "print(counter_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ff7b42dc-f69f-472c-987c-cbcb5e3f2190",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def return_answer_startend_valid(idx):\n",
    "    return_start = 0\n",
    "    return_end = 0\n",
    "    answer_encoding_fast = tokenizerFast(valid_answers_valid[idx]['text'], max_length=MAX_LENGTH, truncation=True, padding=True)\n",
    "    \n",
    "    for a in range(len(valid_encodings_fast['input_ids'][idx]) - len(answer_encoding_fast['input_ids'])):\n",
    "        match = True\n",
    "        for i in range(1, len(answer_encoding_fast['input_ids']) - 1):\n",
    "            if (answer_encoding_fast['input_ids'][i] != valid_encodings_fast['input_ids'][idx][a + i]):\n",
    "                match = False\n",
    "                break\n",
    "            if match:\n",
    "                return_start = a + 1\n",
    "                return_end = a + i + 1\n",
    "                break\n",
    "    return(return_start, return_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "176f0c46-a07a-4b2d-840c-fe8475271070",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "236\n"
     ]
    }
   ],
   "source": [
    "start_positions = []\n",
    "end_positions = []\n",
    "counter = 0\n",
    "\n",
    "for h in range(len(valid_encodings_fast['input_ids'])):\n",
    "    s, e = return_answer_startend_valid(h)\n",
    "    start_positions.append(s)\n",
    "    end_positions.append(e)\n",
    "    \n",
    "    if s == 0:\n",
    "        counter += 1\n",
    "\n",
    "valid_encodings_fast.update({'start_positions': start_positions, 'end_positions': end_positions})\n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cbe65eea-74c8-4930-b0ec-cd36b2ecb6c1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForQuestionAnswering, BertTokenizerFast\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class InputDataset(Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = encodings\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return {\n",
    "            'input_ids': torch.tensor(self.encodings['input_ids'][i]),\n",
    "            'token_type_ids': torch.tensor(self.encodings['token_type_ids'][i]),\n",
    "            'attention_mask': torch.tensor(self.encodings['attention_mask'][i]),\n",
    "            'start_positions': torch.tensor(self.encodings['start_positions'][i]),\n",
    "            'end_positions': torch.tensor(self.encodings['end_positions'][i])\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings['input_ids'])\n",
    "\n",
    "# Assuming 'train_encodings_fast' and 'valid_encodings_fast' are already prepared with start and end positions\n",
    "train_dataset = InputDataset(train_encodings_fast)\n",
    "valid_dataset = InputDataset(valid_encodings_fast)\n",
    "\n",
    "train_data_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "valid_data_loader = DataLoader(valid_dataset, batch_size=1)\n",
    "\n",
    "# Load the pre-trained BERT model for question answering\n",
    "bert_model = BertForQuestionAnswering.from_pretrained(MODEL_PATH)  # MODEL_PATH = \"bert-base-uncased\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "20edd63f-0f49-4b09-b288-7ec29c20d423",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class QAModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(QAModel, self).__init__()\n",
    "        self.bert = bert_model  # Pretrained Bert model\n",
    "        self.drop_out = nn.Dropout(0.1)\n",
    "        self.l1 = nn.Linear(768 * 2, 768 * 2)\n",
    "        self.l2 = nn.Linear(768 * 2, 2)\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            self.drop_out,\n",
    "            self.l1,\n",
    "            nn.LeakyReLU(),\n",
    "            self.l2\n",
    "        )\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
    "        model_output = self.bert(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, output_hidden_states=True)\n",
    "        hidden_states = model_output.hidden_states  # Extract hidden states\n",
    "        out = torch.cat((hidden_states[-1], hidden_states[-3]), dim=-1)  # Concatenate last and second-to-last hidden states\n",
    "        logits = self.linear_relu_stack(out)\n",
    "        start_logits, end_logits = logits.split(1, dim=-1)  # Split logits for start and end\n",
    "        start_logits = start_logits.squeeze(-1)\n",
    "        end_logits = end_logits.squeeze(-1)\n",
    "\n",
    "        return start_logits, end_logits\n",
    "model = QAModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0baa230d-c191-40b2-962b-9f42949b98c2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/lib/python3.11/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "def compute_loss(start_logits, end_logits, start_positions, end_positions):\n",
    "    loss_func = nn.CrossEntropyLoss()\n",
    "    start_loss_val = loss_func(start_logits, start_positions)\n",
    "    end_loss_val = loss_func(end_logits, end_positions)\n",
    "    avg_loss = (start_loss_val + end_loss_val) / 2\n",
    "    return avg_loss\n",
    "\n",
    "def compute_focal_loss(start_logits, end_logits, start_positions, end_positions, gamma):\n",
    "    softmax = nn.Softmax(dim=1)\n",
    "    start_probs = softmax(start_logits)\n",
    "    inv_start_probs = 1 - start_probs\n",
    "    end_probs = softmax(end_logits)\n",
    "    inv_end_probs = 1 - end_probs\n",
    "    log_softmax = nn.LogSoftmax(dim=1)\n",
    "    start_log_probs = log_softmax(start_logits)\n",
    "    end_log_probs = log_softmax(end_logits)\n",
    "    \n",
    "    negative_log_likelihood = nn.NLLLoss()\n",
    "    \n",
    "    focal_loss_start = negative_log_likelihood(torch.pow(inv_start_probs, gamma) * start_log_probs, start_positions)\n",
    "    focal_loss_end = negative_log_likelihood(torch.pow(inv_end_probs, gamma) * end_log_probs, end_positions)\n",
    "    \n",
    "    return (focal_loss_start + focal_loss_end) / 2\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5, weight_decay=2e-2)\n",
    "lr_scheduler = ExponentialLR(optimizer, gamma=0.9)\n",
    "\n",
    "total_accuracy = []\n",
    "total_train_loss = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4d90f5c6-36ef-481f-bf2b-cd2c31c75c8b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_one_epoch(model, dataloader, epoch):\n",
    "    model = model.train()\n",
    "    loss_values = []\n",
    "    accuracy_values = []\n",
    "    batch_counter = 0\n",
    "    for batch in tqdm(dataloader, desc=f'Training Epoch {epoch}'):\n",
    "        optimizer.zero_grad()\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        token_type_ids = batch['token_type_ids'].to(device)\n",
    "        start_positions = batch['start_positions'].to(device)\n",
    "        end_positions = batch['end_positions'].to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        start_logits, end_logits = model(input_ids=input_ids, \n",
    "                                          attention_mask=attention_mask,\n",
    "                                          token_type_ids=token_type_ids)\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = compute_focal_loss(start_logits, end_logits, start_positions, end_positions, gamma=1)\n",
    "        loss_values.append(loss.item())\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Predictions and accuracy calculation\n",
    "        start_preds = torch.argmax(start_logits, dim=1)\n",
    "        end_preds = torch.argmax(end_logits, dim=1)\n",
    "\n",
    "        accuracy_values.append(((start_preds == start_positions).sum() / len(start_preds)).item())\n",
    "        accuracy_values.append(((end_preds == end_positions).sum() / len(end_preds)).item())\n",
    "\n",
    "        batch_counter += 1\n",
    "        if batch_counter == 250 and epoch == 1:\n",
    "            avg_acc = sum(accuracy_values) / len(accuracy_values)\n",
    "            total_accuracy.append(avg_acc)\n",
    "            avg_loss = sum(loss_values) / len(loss_values)\n",
    "            total_train_loss.append(avg_loss)\n",
    "            batch_counter = 0\n",
    "    \n",
    "    lr_scheduler.step()\n",
    "    avg_accuracy = sum(accuracy_values) / len(accuracy_values)\n",
    "    avg_loss = sum(loss_values) / len(loss_values)\n",
    "\n",
    "    return avg_accuracy, avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9116d650-4b4c-4195-8f07-acaeda933ab8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluate_on_model(model, dataloader):\n",
    "    model = model.eval()\n",
    "    loss_values = []\n",
    "    accuracy_values = []\n",
    "    batch_counter = 0\n",
    "    predicted_answers = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc='Evaluating the Model'):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            token_type_ids = batch['token_type_ids'].to(device)\n",
    "            start_positions_true = batch['start_positions'].to(device)\n",
    "            end_positions_true = batch['end_positions'].to(device)\n",
    "\n",
    "            start_logits, end_logits = model(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "            \n",
    "            start_preds = torch.argmax(start_logits, dim=1)\n",
    "            end_preds = torch.argmax(end_logits, dim=1)\n",
    "            \n",
    "            predicted_answer = tokenizerFast.convert_tokens_to_string(tokenizerFast.convert_ids_to_tokens(input_ids[0][start_preds:end_preds]))\n",
    "            true_answer = tokenizerFast.convert_tokens_to_string(tokenizerFast.convert_ids_to_tokens(input_ids[0][start_positions_true[0]:end_positions_true[0]]))\n",
    "            \n",
    "            predicted_answers.append([predicted_answer, true_answer])\n",
    "    \n",
    "    return predicted_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e46558eb-0e2a-4202-b0c8-e914b3007f62",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: evaluate in ./.local/lib/python3.11/site-packages (0.4.3)\n",
      "Requirement already satisfied: datasets>=2.0.0 in ./.local/lib/python3.11/site-packages (from evaluate) (3.1.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/lib/python3.11/site-packages (from evaluate) (1.24.3)\n",
      "Requirement already satisfied: dill in /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/lib/python3.11/site-packages (from evaluate) (0.3.6)\n",
      "Requirement already satisfied: pandas in /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/lib/python3.11/site-packages (from evaluate) (2.0.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in ./.local/lib/python3.11/site-packages (from evaluate) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in ./.local/lib/python3.11/site-packages (from evaluate) (4.67.0)\n",
      "Requirement already satisfied: xxhash in /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/lib/python3.11/site-packages (from evaluate) (2.0.2)\n",
      "Requirement already satisfied: multiprocess in /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/lib/python3.11/site-packages (from evaluate) (0.70.14)\n",
      "Requirement already satisfied: fsspec[http]>=2021.05.0 in ./.local/lib/python3.11/site-packages (from evaluate) (2024.9.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in ./.local/lib/python3.11/site-packages (from evaluate) (0.26.2)\n",
      "Requirement already satisfied: packaging in /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/lib/python3.11/site-packages (from evaluate) (23.1)\n",
      "Requirement already satisfied: filelock in /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/lib/python3.11/site-packages (from datasets>=2.0.0->evaluate) (3.9.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in ./.local/lib/python3.11/site-packages (from datasets>=2.0.0->evaluate) (18.0.0)\n",
      "Requirement already satisfied: aiohttp in /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/lib/python3.11/site-packages (from datasets>=2.0.0->evaluate) (3.8.5)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/lib/python3.11/site-packages (from datasets>=2.0.0->evaluate) (6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./.local/lib/python3.11/site-packages (from huggingface-hub>=0.7.0->evaluate) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/lib/python3.11/site-packages (from requests>=2.19.0->evaluate) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/lib/python3.11/site-packages (from requests>=2.19.0->evaluate) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/lib/python3.11/site-packages (from requests>=2.19.0->evaluate) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/lib/python3.11/site-packages (from requests>=2.19.0->evaluate) (2023.7.22)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/lib/python3.11/site-packages (from pandas->evaluate) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/lib/python3.11/site-packages (from pandas->evaluate) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/lib/python3.11/site-packages (from pandas->evaluate) (2023.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/lib/python3.11/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (22.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/lib/python3.11/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.2)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/lib/python3.11/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/lib/python3.11/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.8.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/lib/python3.11/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/lib/python3.11/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.2.0)\n",
      "Requirement already satisfied: six>=1.5 in /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "231e75b4-15c2-488f-af48-e78c2c011403",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: huggingface_hub in ./.local/lib/python3.11/site-packages (0.26.2)\n",
      "Requirement already satisfied: filelock in /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/lib/python3.11/site-packages (from huggingface_hub) (3.9.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./.local/lib/python3.11/site-packages (from huggingface_hub) (2024.9.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/lib/python3.11/site-packages (from huggingface_hub) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/lib/python3.11/site-packages (from huggingface_hub) (6.0)\n",
      "Requirement already satisfied: requests in ./.local/lib/python3.11/site-packages (from huggingface_hub) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in ./.local/lib/python3.11/site-packages (from huggingface_hub) (4.67.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./.local/lib/python3.11/site-packages (from huggingface_hub) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/lib/python3.11/site-packages (from requests->huggingface_hub) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/lib/python3.11/site-packages (from requests->huggingface_hub) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/lib/python3.11/site-packages (from requests->huggingface_hub) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/lib/python3.11/site-packages (from requests->huggingface_hub) (2023.7.22)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c94617da-a5d5-417b-9493-335113f3beda",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: datasets in ./.local/lib/python3.11/site-packages (3.1.0)\n",
      "Requirement already satisfied: filelock in /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/lib/python3.11/site-packages (from datasets) (3.9.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/lib/python3.11/site-packages (from datasets) (1.24.3)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in ./.local/lib/python3.11/site-packages (from datasets) (18.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/lib/python3.11/site-packages (from datasets) (0.3.6)\n",
      "Requirement already satisfied: pandas in /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/lib/python3.11/site-packages (from datasets) (2.0.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in ./.local/lib/python3.11/site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in ./.local/lib/python3.11/site-packages (from datasets) (4.67.0)\n",
      "Requirement already satisfied: xxhash in /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/lib/python3.11/site-packages (from datasets) (2.0.2)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/lib/python3.11/site-packages (from datasets) (0.70.14)\n",
      "Requirement already satisfied: fsspec[http]<=2024.9.0,>=2023.1.0 in ./.local/lib/python3.11/site-packages (from datasets) (2024.9.0)\n",
      "Requirement already satisfied: aiohttp in /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/lib/python3.11/site-packages (from datasets) (3.8.5)\n",
      "Requirement already satisfied: huggingface-hub>=0.23.0 in ./.local/lib/python3.11/site-packages (from datasets) (0.26.2)\n",
      "Requirement already satisfied: packaging in /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/lib/python3.11/site-packages (from datasets) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/lib/python3.11/site-packages (from datasets) (6.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/lib/python3.11/site-packages (from aiohttp->datasets) (22.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/lib/python3.11/site-packages (from aiohttp->datasets) (2.0.4)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/lib/python3.11/site-packages (from aiohttp->datasets) (6.0.2)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/lib/python3.11/site-packages (from aiohttp->datasets) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/lib/python3.11/site-packages (from aiohttp->datasets) (1.8.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/lib/python3.11/site-packages (from aiohttp->datasets) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/lib/python3.11/site-packages (from aiohttp->datasets) (1.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./.local/lib/python3.11/site-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/lib/python3.11/site-packages (from requests>=2.32.2->datasets) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/lib/python3.11/site-packages (from requests>=2.32.2->datasets) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/lib/python3.11/site-packages (from requests>=2.32.2->datasets) (2023.7.22)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/lib/python3.11/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/lib/python3.11/site-packages (from pandas->datasets) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/lib/python3.11/site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2aecea4b-d0f0-43b3-b0c0-f629c7072bea",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: jiwer in ./.local/lib/python3.11/site-packages (3.0.5)\n",
      "Requirement already satisfied: click<9.0.0,>=8.1.3 in ./.local/lib/python3.11/site-packages (from jiwer) (8.1.7)\n",
      "Requirement already satisfied: rapidfuzz<4,>=3 in ./.local/lib/python3.11/site-packages (from jiwer) (3.10.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install jiwer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "af5f3f96-2843-463e-a623-acd2fe054d53",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 2320/2320 [03:57<00:00,  9.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.7350658097400747      Train Loss: 0.680211986546758\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating the Model: 100%|██████████| 15875/15875 [00:59<00:00, 266.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.756976377952756]\n",
      "['this', 'is', 'a', 'sentence', '.']\n",
      "this is a sentence.\n"
     ]
    }
   ],
   "source": [
    "from evaluate import load\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "wer_metric = load(\"wer\")  # Renamed 'wer' to 'wer_metric'\n",
    "EPOCH_COUNT = 1  # Renamed 'EPOCHS' to 'EPOCH_COUNT'\n",
    "model.to(device)\n",
    "\n",
    "wer_scores = []  # Renamed 'wer_list' to 'wer_scores'\n",
    "\n",
    "for epoch_num in range(EPOCH_COUNT):  # Renamed 'epoch' to 'epoch_num'\n",
    "    # Training the model\n",
    "    train_accuracy, train_loss = train_one_epoch(model, train_data_loader, epoch_num + 1)  # Renamed variables\n",
    "    print(f\"Train Accuracy: {train_accuracy}      Train Loss: {train_loss}\")\n",
    "    \n",
    "    # Evaluating the model\n",
    "    answer_pairs = evaluate_on_model(model, valid_data_loader)  # Renamed 'answer_list' to 'answer_pairs'\n",
    "    predicted_answers = []  # Renamed 'pred_answers' to 'predicted_answers'\n",
    "    true_answers = []  # Renamed 'true_answers' to 'true_answers'\n",
    "\n",
    "    for i in range(len(answer_pairs)):\n",
    "        # Handling empty answers\n",
    "        if len(answer_pairs[i][0]) == 0:\n",
    "            answer_pairs[i][0] = \"$\"\n",
    "        if len(answer_pairs[i][1]) == 0:\n",
    "            answer_pairs[i][1] = \"$\"\n",
    "        \n",
    "        predicted_answers.append(answer_pairs[i][0])\n",
    "        true_answers.append(answer_pairs[i][1])\n",
    "\n",
    "    # Calculate WER\n",
    "    wer_score = wer_metric.compute(predictions=predicted_answers, references=true_answers)  # Renamed 'wer' to 'wer_metric'\n",
    "    wer_scores.append(wer_score)\n",
    "\n",
    "# Print WER scores after the loop\n",
    "print(wer_scores)\n",
    "\n",
    "# Example Tokenization\n",
    "tokens = tokenizerFast.tokenize(\"This is a sentence.\")\n",
    "print(tokens)\n",
    "\n",
    "output = tokenizerFast.convert_tokens_to_string(tokens)\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f4cf217-6565-4053-b40e-46e67145ec97",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6dbf500-5056-4c52-8c25-f2d8b34a9a0f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
